\documentclass[twocolumn]{article}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{amsmath, amssymb}
\usepackage[style=numeric-comp,useprefix,hyperref,backend=bibtex]{biblatex}
\graphicspath{{../img/}}
\usepackage{endnotes}
\usepackage{float}
\title{Wine Project Report}
\author{Ruggero Nocera (SXXXXX1) \\ Quarta Matteo (SXXXXXX)}
\date{}


\begin{document}

\maketitle
\tableofcontents


\section{Preliminary Data Analysis}
\subsection{Feature Distribution}

Before discussing the varius models and techniques that can give us robust result, we shortly discuss how the feature of our dataset are distributed.

\begin{figure}[H]
    \caption{Histogram of Class' Features}
    {\includegraphics[width=\linewidth]{dist.jpg}}
    \label{featureshist}
\end{figure}

We start by noticing that the order of magnitude of some features can differ much from one another: for example, if we look at feature 5, it seems to span from 0.0 to as much as 0.6, but feature 7 instead spans two whole orders, occasionally being over 200, and with some outliers even double as much. An interesting observation is that some features do look gaussian distributed or regular enough to be described by a combination of gaussian distributions.

This may hint that some dimensionality reduction will be strongly biased towards features like 7, and that normalization can be taken as a valid pre-processing technique.

We also notice that our dataset is unbalanced: the quantity of bad wine is roughly twice the quantity of good one. This has to be taken into account when developing models, but we will be trating them as if the dataset is balanced, that is considering and equal probability of good wine and bad wine, occasionally rebalancing scores.

Let's apply a 2D projection of our data to further discuss it.

\begin{figure}[H]
    \caption{2D-PCA Projection}
    {\includegraphics[width=\linewidth]{2DRAW.png}}
    \label{2DRAW}
\end{figure}

The features look highly correlated and fairly regularly distributed: this may be a hint that some gassuain models may perform well enough, like Full-Covariang MVG Classifiers or Tied-Covariance ones, but some other with linear decision boundaries, like linear SVMs, may perform poorly.

\begin{figure}[H]
    \caption{2D-PCA Projection, Normalized Data}
    {\includegraphics[width=\linewidth]{2DNorm.png}}
    \label{2DNORM}
\end{figure}

After normalization (Z-Normalization, mean $\mu = 0$ and St.D. $\sigma = 1$ ) our points do get apart but not significantly: we could expect linear models to still perform poorly and gaussian classifiers to get slightly worse. On the other hand, Gaussian Mixture Models look like a more interesting choice.

Lastly we take a look at normalized and whitened data.

\begin{figure}[H]
    \caption{2D-PCA Projection, Whitened Data}
    {\includegraphics[width=\linewidth]{2DWhitened.png}}
    \label{2DWHI}
\end{figure}

Here the results are a bit more interesting: points do get separeted just like after normalization, but same class points stay close to each other while different class get more apart. In this scenario, it looks like even linear model might have a chance to achieve a decent performance.

\subsection{Dimensionality Reduction}

We will shortly decide, by running some non-optimized models, which pre-processing techniques are useful and which are not. 

We will be reporting the best results for each classifiers, extracted from a 3 or 5 Fold Validation approach. The decision rule, since this is a binary problem, will be set to $\displaystyle t = \log(\frac{\pi_T}{1-\pi_T}) \lessgtr 0 $, where $\pi_T$ is assumed $ = 0.5$ and it is the prior probability of class Good.

Let's start with Gaussian Classifiers: Full-Covariance, Tied-Covariance, and Naive-Bayes.


    \begin{table}
    \begin{tabular}{||c|c|c||}
        \hline
        PCA & DCF & minDCF \\
        \hline
        \hline
        A & B & C \\
        A & B & C \\
    \end{tabular}
\end{table}
\caption{My Table}



% Logistic Regression results
\newpage
\begin{table}
\input{../data/logreg_acc.tex}
\input{../data/logreg_pca_acc.tex}
\input{../data/logreg_pcan_acc.tex}
\end{table}

% Linear SVM results
\newpage
\begin{table}
\input{../data/svm_linear_acc.tex}
\input{../data/svm_linpca_acc.tex}
\input{../data/svm_linpcan_acc.tex}
\end{table}
\end{document}