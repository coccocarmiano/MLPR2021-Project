\documentclass[12pt, twocolumn]{article}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{amsmath, amssymb}
\usepackage{cuted}
\usepackage[margin=0.7in]{geometry}
\usepackage[style=numeric-comp,useprefix,hyperref,backend=bibtex]{biblatex}
\graphicspath{{../img/}}
\usepackage{endnotes}
\usepackage{float}
\title{Wine Project Report}
\author{Ruggero Nocera (SXXXXX1) \\ Quarta Matteo (SXXXXXX)}
\date{}


\begin{document}

\maketitle
\begin{strip}
    {\bf Abstract}
    This paper is an analysis of the effectiveness of the various models studied during the course applied to a binary classification model.
    The dataset is a transformed version of the one provided in {\it Modeling wine preferences by data mining from physicochemical properties}
    from {\it Decision Support Systems, Elsevier} (P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.), where grades span from 0 to 10. 
    Grade 6 has been removed, grades above 6 have been mapped to class 1 and grades below 6 to class 0.
    Due to limited time and computanional power, results may be just close-to or far-from optimal, depending on the time required to train a model.
\end{strip}
\tableofcontents


\section{Preliminary Data Analysis}
\subsection{Feature Distribution}

Before discussing the model, their implementation and their effectiveness we briefly take a look at how the features are distributed.
For convenience we shall now report the legend just one, but keep in mind that in all pictures red color is associated to class 0 (which we will be referring to as class {\it Bad}) and green color is associated to class 1 (which will be class {\it Good}).

\begin{figure}[H]
    \caption{Histogram of Class' Features}
    {\includegraphics[width=\linewidth]{dist.jpg}}
    \label{fig:disthist}
\end{figure}

First of all, our training dataset is unbalanced. 
In the next pages we will be classifying samples obtained from a K-Fold\footnotemark Validation approach, using a theoretical threshold given by:

$$ t = -\log{\frac{\pi}{1-\pi}} $$

For the threshold to be optimal, we should use the empirical prior $\pi \approx .33$ based on a frequentist approach;
Instead we will be using a non-optimal prior $\tilde{\pi} = .5$ as it is the application we are going to be targeting
.
\footnotetext{K varies through models. For fast ones, 5 or 10 is used. For slower ones, 3 is used.}

Coming back to features distributions, some things are to be noticed.
While some features are similar between class Good and class Bad (see feature {\it 1}) others differ substantially and can be very helpful in discriminating samples (see feature {\it 8}).

Moreover, the features are distributed in various ways: while some do look pretty Gaussian (see feature {\it 9}) and others are instead fairly regular (see feature {\it 5}) and could thus be well estimated by Gaussian models\footnotemark , other act in a more irregular way.

\footnotetext{Meaning both Gaussian Classifiers and Gaussian Mixture Models}.

To take a closer look we now project the data on the two dimensional plane. 
We will be using 3 methods to do so: PCA\footnotemark , Normalization + PCA, Normalization + Whitening.

\footnotetext{Without data centering to appreciate the correlation}

\begin{figure}[H]
    \caption{2D-PCA Projection}
    {\includegraphics[width=\linewidth]{2DRAW.png}}
    \label{2DRAW}
\end{figure}

The points are very close one another, we thus expect linear models to be not so effective compared to others.
The data is pretty {\it circularly} distributed, so correlation may not play an important role in discriminating samples.

\begin{figure}[H]
    \caption{2D-PCA Projection, Normalized Data}
    {\includegraphics[width=\linewidth]{2DNorm.png}}
    \label{2DNORM}
\end{figure}

The normalized projection seems to split data in two clusters, each containing some samples of either class, but some points of different class seem to get far apart from the other.
So normalization is a technique worth trying.

Note that for Normalization we refer to {\it Z-Normalization}. 
From some early tests we found that {\it Min-Max} normalization was not very effective, and with {\it Gaussianization} centering and scaling data in the same way as Z while being slower, we decided to use this method.

Lastly we take a look at normalized and whitened data.

\begin{figure}[H]
    \caption{2D-PCA Projection, Whitened Data}
    {\includegraphics[width=\linewidth]{2DWhitened.png}}
    \label{2DWHI}
\end{figure}

Results do look interesting: points to get much apart, even if we can spot many outliers.
With the distribution being this way, we could expect even linear models to achieve decent results.


\section{Pre-Processing Analysis}

In this section we will run some dummy\footnotemark models to infer wheter pre-processing, by the means of PCA, normalization, etc. can be useful or worth trying.

\footnotetext{Referring to models requiring parameter tuning or score recalibration}

For now we will run all possible combinations of preprocessing (Raw, Normalized, Whitened) with PCA reductions (2-PCA, 3-PCA, ...).

\subsection{Pre-Processing MVG Classifiers}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 9 & 0.401 & {\bf 0.362}  \\
            Normalized &  7 & 0.464 & 0.420 \\
            Whitened & 2 & 0.430 & 0.410 \\
            \hline
    \end{tabular}
    \caption{Full-Covariance MVG - Best Results}
    \label{fullcovtab}
\end{table}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 7 & {\bf 0.375} & 0.366  \\
            Normalized &  9 & {\bf 0.375} & 0.371 \\
            Whitened & 11 & 0.402 & 0.401 \\
            \hline
    \end{tabular}
    \caption{Tied-Covariance MVG - Best Results}
    \label{tiedcovtab}
\end{table}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 7 & 0.383 & 0.366  \\
            Normalized &  10 & 0.429 & 0.391 \\
            Whitened & 5 & 0.402 & 0.386 \\
            \hline
    \end{tabular}
    \caption{Naive-Bayes MVG - Best Results}
    \label{naivetab}
\end{table}

The result validate some of our assumptions and invalidate others.
By looking at the DCF\footnotemark values, our best model seems to be the Tied-Covariance ones.
\footnotetext{By DCF we actually mean the {\it normalized} DCF, considering a prior $\pi = .5$ and equal costs}
We are not very surprised to se that the Naive Bayes does sometimes outperform the Full-Covariance model as we noticed in Figure \ref{2DRAW} that some features are not very correlated.
The same can said about the the Tied covariance by looking at the same projection or feature {\it 8} of Figure \ref{fig:disthist}.

Normalization and Whitening do not help, they harm, sometimes even significantly the classification, while it looks like PCA can be of modest help.

An interesting fact that is that while the Full-Covariance model looks like the worse perfoming one, it is the model with the lowest minDCF values.
While it is not an important difference compared with other models, it is the one with the biggest difference between the DCF and minDCF values, hinting that score calibration could be required.

\subsection{Pre-Processing for GMMs}

For GMMs we decide the number of components (for this dummy execution 4 was picked arbitrairly).
We start with identity covariance matrices and by placing the means near the dataset mean.\footnotemark
\footnotetext{The starting points are used in the same way also in the optimization part.}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & No & 0.410 & {\bf 0.394}  \\
            Normalized &  9 & {\bf 0.407} & 0.402 \\
            Whitened & 4 & 0.429 & 0.420 \\
            \hline
    \end{tabular}
    \caption{GMM - Best Results}
\end{table}

As we would expect after the results of the Gaussian Classifiers, also here whitening does not look like a good choice and normalization looks ineffective.
High dimensionality still looks preferred.

The DCF values are close to the minDCF values, score calibration may be not necessary.

\subsection{Pre-Processing for Polynomial Kernel SVMs}

The polynomial kernel mapping $\phi(\cdot)$ we will use is so defined:

\begin{center}
    \begin{align}
        \phi({\bf x}) & = \begin{bmatrix}
                            vec({\bf x}{\bf x}^T) \\
                            \sqrt{2}{\bf x} \\
                            1 \\
                      \end{bmatrix}
    \end{align}
    \label{phi}
\end{center}

And we will be using the kernel function so described:

$$ \phi({\bf x}_1)^T\phi({\bf x}_2) = k({\bf x_1}, {\bf x_2}) = $$
$$ = ({\bf x}_1^T{\bf x}_2 + c)^d + b = ({\bf x}_1^T{\bf x}_2+1)^2+0.5$$

The formulation of \ref{phi} would require us to strictly use $ c = 1 $, as we do in our dummy model, but later on this will be an hyperparameter to be optimized.

The results are interesting:

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 7 & 0.554 &  0.545  \\
            Normalized & 6 & {\bf 0.393} &  {\bf 0.381}  \\
            Whitened & 11 & 0.399 &  0.393  \\
            \hline
    \end{tabular}
    \caption{Polynomail Kernel SVM - Best Results}
\end{table}

While competitiveness with other models has to be made later, when each one will be fully used, here we see an interesting change:
normalization greatly improves our classification.
While a lower PCA has obtained the lowest DCF, it is not significantly lower than the one obtained with higher dimensionality, so we can say that here it's normalization helping us out.
Whitening, again, does not make any difference, as the results are same or close to the ones obtained with normalization only.

Notice also how scores seems well calibration with our theoretical threshold even if they do not have a probabilistic interpretation.

\subsection{Pre-Processing for RBF Kernel SVMs}

The dummy RBF function used here is the following:

$$\displaystyle k({\bf x_1}, {\bf x_2}) = e^{-\gamma||{\bf x}_1 - {\bf x}_2||^2} + b = e^{-0.05||{\bf x}_1 - {\bf x}_2||^2} + 0.05 $$

Small values of $\gamma$ and $b$ were picked to avoid numerical issues when using non-normalized models.

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
                Raw & 10 & 0.588 & 0.579 \\ 
                Normalized & 11 & {\bf 0.356} & {\bf 0.352} \\ 
                Whitened & 11 & {\bf 0.356} & {\bf 0.352} \\ 
            \hline
    \end{tabular}
    \caption{RBF Kernel SVM - Best Results}
\end{table}

Just like polynomial kernel, RBF prefers high dimensionality , so further testing in this direction is required.
Again, normalization plays an important role while whitening seems to have no effect whatsoever and scores look extremely well calibrated.

Just like polynomial kernel SVMs, here normalization has no effect whatsoever whilte normalization greatly improves our performance.
The RBF Kernel works significantly better when working with all or most components. 

\section{Optimizing Models}
\subsection{Optimizing MVG Classifiers}

Since MVG Classifiers do not have parameters to be tuned, we are going to discuss only the possibility of recalibrating scores.
In particular we will not discuss the Tied Covariance model, as based on its DCF values, the scores look already well calibrated.



\end{document}