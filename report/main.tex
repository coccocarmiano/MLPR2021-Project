\documentclass[12pt, twocolumn]{article}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{amsmath, amssymb}
\usepackage{cuted}
\usepackage{amsbsy}
\usepackage[margin=0.7in]{geometry}
\usepackage[style=numeric-comp,useprefix,hyperref,backend=bibtex]{biblatex}
\graphicspath{{../img/}}
\usepackage{endnotes}
\usepackage{float}
\title{Wine Project Report}
\author{Ruggero Nocera (SXXXXX1) \\ Quarta Matteo (SXXXXXX)}
\date{}


\begin{document}

\maketitle
\begin{strip}
    {\bf Abstract}
    This paper is an analysis of the effectiveness of the various models studied during the course applied to a binary classification model.
    The dataset is a transformed version of the one provided in {\it Modeling wine preferences by data mining from physicochemical properties}
    from {\it Decision Support Systems, Elsevier} (P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.), where grades span from 0 to 10. 
    Grade 6 has been removed, grades above 6 have been mapped to class 1 and grades below 6 to class 0.
    Due to limited time and computanional power, results may be just close-to or far-from optimal, depending on the time required to train a model.
\end{strip}
\tableofcontents


\section{Preliminary Data Analysis}
\subsection{Feature Distribution}

Before discussing the model, their implementation and their effectiveness we briefly take a look at how the features are distributed.
For convenience we shall now report the legend just one, but keep in mind that in all pictures red color is associated to class 0 (which we will be referring to as class {\it Bad}) and green color is associated to class 1 (which will be class {\it Good}).

\begin{figure}[H] 
    \caption{Histogram of Class' Features}
    \label{fig:disthist}
    {\includegraphics[width=\linewidth]{dist.jpg}}
\end{figure}

First of all, our training dataset is unbalanced. 
In the next pages we will be classifying samples obtained from a K-Fold\footnotemark Validation approach, using a theoretical threshold given by:

$$ t = -\log{\frac{\pi}{1-\pi}} $$

For the threshold to be optimal, we should use the empirical prior $\pi \approx .33$ based on a frequentist approach;
Instead we will be using a non-optimal prior $\tilde{\pi} = .5$ as it is the application we are going to be targeting
.
\footnotetext{K varies through models. For fast ones, 5 or 10 is used. For slower ones, 3 is used.}

Coming back to features distributions, some things are to be noticed.
While some features are similar between class Good and class Bad (see feature {\it 1}) others differ substantially and can be very helpful in discriminating samples (see feature {\it 8}).

Moreover, the features are distributed in various ways: while some do look pretty Gaussian (see feature {\it 9}) and others are instead fairly regular (see feature {\it 5}) and could thus be well estimated by Gaussian models\footnotemark , other act in a more irregular way.

\footnotetext{Meaning both Gaussian Classifiers and Gaussian Mixture Models}.

To take a closer look we now project the data on the two dimensional plane. 
We will be using 3 methods to do so: PCA\footnotemark , Normalization + PCA, Normalization + Whitening.

\footnotetext{Without data centering to appreciate the correlation}

\begin{figure}[H]     
    {\includegraphics[width=\linewidth]{2DRAW.png}}
    \caption{2D-PCA Projection}
    \label{fig:2DRAW}
\end{figure}

The points are very close one another, we thus expect linear models to be not so effective compared to others.
The data is pretty {\it circularly} distributed, so correlation may not play an important role in discriminating samples.

\begin{figure}[H] 
    {\includegraphics[width=\linewidth]{2DNorm.png}}
    \caption{2D-PCA Projection, Normalized Data}
    \label{fig:2DNORM}
\end{figure}

The normalized projection seems to split data in two clusters, each containing some samples of either class, but some points of different class seem to get far apart from the other.
So normalization is a technique worth trying.

Note that for Normalization we refer to {\it Z-Normalization}. 
From some early tests we found that {\it Min-Max} normalization was not very effective, and with {\it Gaussianization} centering and scaling data in the same way as Z while being slower, we decided to use this method.

Lastly we take a look at normalized and whitened data.

\begin{figure}[H] 
    {\includegraphics[width=\linewidth]{2DWhitened.png}}
    \label{fig:2DWHI}
    \caption{2D-PCA Projection, Whitened Data}
\end{figure}

Results do look interesting: points to get much apart, even if we can spot many outliers.
With the distribution being this way, we could expect even linear models to achieve decent results.


\section{Pre-Processing Analysis}

In this section we will run some dummy\footnotemark models to infer wheter pre-processing, by the means of PCA, normalization, etc. can be useful or worth trying.

\footnotetext{Referring to models requiring parameter tuning or score recalibration}

For now we will run all possible combinations of preprocessing (Raw, Normalized, Whitened) with PCA reductions (2-PCA, 3-PCA, ...).

\subsection{Pre-Processing MVG Classifiers}

\begin{table}[H] 
    \centering
    \begin{tabular}{||c|c|c|c||}
        \hline
        Type & PCA & DCF & minDCF \\
        \hline
        \hline
        Raw & 9 & 0.401 & {\bf 0.362}  \\
        Normalized &  7 & 0.464 & 0.420 \\
        Whitened & 2 & 0.430 & 0.410 \\
        \hline
    \end{tabular}
    \caption{Full-Covariance MVG - Best Results}
    \label{fullcovtab}
\end{table}

    
\begin{table}[H] 
    \centering
    \begin{tabular}{||c|c|c|c||}
        \hline
        Type & PCA & DCF & minDCF \\
        \hline
        \hline
        Raw & 7 & {\bf 0.375} & 0.366  \\
        Normalized &  9 & {\bf 0.375} & 0.371 \\
        Whitened & 11 & 0.402 & 0.401 \\
        \hline
    \end{tabular}
    \caption{Tied-Covariance MVG - Best Results}
    \label{tiedcovtab}
\end{table}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 7 & 0.383 & 0.366  \\
            Normalized &  10 & 0.429 & 0.391 \\
            Whitened & 5 & 0.402 & 0.386 \\
            \hline
    \end{tabular}
    \caption{Naive-Bayes MVG - Best Results}
    \label{naivetab}
\end{table}

The result validate some of our assumptions and invalidate others.
By looking at the DCF\footnotemark values, our best model seems to be the Tied-Covariance ones.
\footnotetext{By DCF we actually mean the {\it normalized} DCF, considering a prior $\pi = .5$ and equal costs}
We are not very surprised to se that the Naive Bayes does sometimes outperform the Full-Covariance model as we noticed in Figure \ref{fig:2DRAW} that some features are not very correlated.
The same can said about the the Tied covariance by looking at the same projection or feature {\it 8} of Figure \ref{fig:disthist}.

Normalization and Whitening do not help, they harm, sometimes even significantly the classification, while it looks like PCA can be of modest help.

An interesting fact that is that while the Full-Covariance model looks like the worse perfoming one, it is the model with the lowest minDCF values.
While it is not an important difference compared with other models, it is the one with the biggest difference between the DCF and minDCF values, hinting that score calibration could be required.

\subsection{Pre-Processing for GMMs}

For GMMs we decide the number of components (for this dummy execution 4 was picked arbitrairly).
We start with identity covariance matrices and by placing the means near the dataset mean.\footnotemark
\footnotetext{The starting points are used in the same way also in the optimization part.}

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & No & 0.410 & {\bf 0.394}  \\
            Normalized &  9 & {\bf 0.407} & 0.402 \\
            Whitened & 4 & 0.429 & 0.420 \\
            \hline
    \end{tabular}
    \caption{GMM - Best Results}
    \label{tab:gmmresults}
\end{table}

As we would expect after the results of the Gaussian Classifiers, also here whitening does not look like a good choice and normalization looks ineffective.
High dimensionality still looks preferred.

The DCF values are close to the minDCF values, score calibration may be not necessary.

\subsection{Pre-Processing for Polynomial Kernel SVMs}

The polynomial kernel mapping $\phi(\cdot)$ we will use is so defined:

\begin{center}
    \begin{align}
        \phi({\bf x}) & = \begin{bmatrix}
                            vec({\bf x}{\bf x}^T) \\
                            \sqrt{2}{\bf x} \\
                            1 \\
                      \end{bmatrix}
    \end{align}
\end{center}

And we will be using the kernel function so described:

$$ \phi({\bf x}_1)^T\phi({\bf x}_2) = k({\bf x_1}, {\bf x_2}) = $$
$$ = ({\bf x}_1^T{\bf x}_2 + c)^d + b = ({\bf x}_1^T{\bf x}_2+1)^2+0.5$$

This formulation would require us to strictly use $ c = 1 $, as we do in our dummy model, but later on this will be an hyperparameter to be optimized.

The results are interesting:

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 7 & 0.554 &  0.545  \\
            Normalized & 6 & {\bf 0.393} &  {\bf 0.381}  \\
            Whitened & 11 & 0.399 &  0.393  \\
            \hline
    \end{tabular}
    \caption{Polynomail Kernel SVM - Best Results}
\end{table}

While competitiveness with other models has to be made later, when each one will be fully used, here we see an interesting change:
normalization greatly improves our classification.
While a lower PCA has obtained the lowest DCF, it is not significantly lower than the one obtained with higher dimensionality, so we can say that here it's normalization helping us out.
Whitening, again, does not make any difference, as the results are same or close to the ones obtained with normalization only.

Notice also how scores seems well calibration with our theoretical threshold even if they do not have a probabilistic interpretation.

\subsection{Pre-Processing for RBF Kernel SVMs}

The dummy RBF function used here is the following:

$$\displaystyle k({\bf x_1}, {\bf x_2}) = e^{-\gamma||{\bf x}_1 - {\bf x}_2||^2} + b = e^{-0.05||{\bf x}_1 - {\bf x}_2||^2} + 0.05 $$

Small values of $\gamma$ and $b$ were picked to avoid numerical issues when using non-normalized models.

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
                Raw & 10 & 0.588 & 0.579 \\ 
                Normalized & 11 & {\bf 0.356} & {\bf 0.352} \\ 
                Whitened & 11 & {\bf 0.356} & {\bf 0.352} \\ 
            \hline
    \end{tabular}
    \caption{RBF Kernel SVM - Best Results}
\end{table}

Just like polynomial kernel, RBF prefers high dimensionality , so further testing in this direction is required.
Again, normalization plays an important role while whitening seems to have no effect whatsoever and scores look extremely well calibrated.

Just like polynomial kernel SVMs, here normalization has no effect whatsoever whilte normalization greatly improves our performance.
The RBF Kernel works significantly better when working with all or most components. 

We are not yet discussing perfomance in detail but so far, this is our most promising model.

\subsection{Pre-Processing for Logistic Regression Model}

Regarding the Logistic Regression, we will consider both linear and quadratic model.

\subsubsection{Linear Logistic Regression}

In the Linear model, dimensionality reduction through PCA and prepocessing through normalization
helped to obtain lower minimum DCF compared to raw features. As the Gaussian Classifiers, only a small number
of features can be removed.

Only results with already optimized hyperparameters are reported.
\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw ($\lambda = 0.01$) & 10 & 0.369 &  0.342  \\
            Normalized ($\lambda = 0.0001$) & 9 & 0.369 &  {\bf 0.336}  \\
            Whitened ($\lambda = 0.001$) & 6 & 0.554 &  0.537  \\
            \hline
    \end{tabular}
    \caption{Linear Logistic Regression - Best Results}
\end{table}

Whitening didn't help so much to improve results, so it won't be considered in further analysis.
\subsubsection{Quadratic Logistic Regression}

In the Quadratic Logistic Regression, after the preprocessing stage, the features space
was expanded through 
$$ {\bf \boldsymbol{\phi}(x)} = {
    \begin{bmatrix}
    vec\langle{\bf xx^T}\rangle\\
    {\bf x}
    \end{bmatrix}}
$$ 

In contrast to linear model, whitening helped as well as normalization, PCA
was useful in both cases, with and without preprocessing, but with a notable difference
in the number of features used to obtain following results.

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c||}
            \hline
            Type & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw ($\lambda = 0.1$) & 6 & 0.385 &  0.366  \\
            Normalized ($\lambda = 0.001$) & 10 & 0.324 &  {\bf 0.307}  \\
            Whitened ($\lambda = 0.001$) & 10 & 0.324 &  {\bf 0.307}  \\
            \hline
    \end{tabular}
    \caption{Quadratic Logistic Regression - Best Results}
\end{table}

\subsection{Pre-Processing for Linear SVM}

The Linear SVM model performed better with normalization and whitening rather than raw features.
Applying PCA to raw features and whitened ones was useful to obtain low values of minimum DCF, in
contrast to normalization which performed better without dimensionality reduction.

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c|c|c|c||}
            \hline
            Type & K & C & PCA & DCF & minDCF \\
            \hline
            \hline
            Raw & 100 & 0.1 & 8 & 0.369 &  0.363  \\
            Normalized & 0.1 & 1.0 & 11 & 0.344 &  {\bf 0.336}  \\
            Whitened & 1.0 & 0.1 & 10 & 0.366 &  {\bf 0.336}  \\
            \hline
    \end{tabular}
    \caption{Linear SVM - Best Results}
\end{table}

\section{Optimizing Models}
\subsection{Optimizing MVG Classifiers}

Since the MVG Classifiers we will be only briefly discussing score calibration.
In particular we will focus on Full-Covariance and Naive Bayes MVG Classifiers, as Tied Covariance already is very close to being optimal.

The scores will be rebalanced using logistic regression as it follows:


\begin{equation}
    f(s) = \alpha s + \beta
\end{equation}

The scores are the ones that produced the best result for both models as in table \ref{fullcovtab} and \ref{naivetab}. 

\begin{table}[H]
    \centering
        \begin{tabular}{||c|c|c||}
            \hline
            minDCF & $DCF_{Before}$ & $DCF_{After}$ \\
            \hline
            \hline
            0.362 & 0.401 & \textcolor{green}{\bf0.390} \\ 
            0.367 & {\bf 0.383} & \textcolor{red}{\bf 0.412} \\ 
            \hline
    \end{tabular}
    \caption{MVG Recalibrated Scores DCFs}
    \label{mvgcalibration}
\end{table}

Recalibration heavily damages our Naive Bayes classifiers so it's not a good choice.
The Full Covariance one has a modest DCF decrement, but it's still worse than the plain Naive Bayes one.
Lastly, by looking at table \ref{tiedcovtab}, we notice that both models perform worse than our Tied Covariance one, so we will be discarding them in our final evaluation.

\subsection{Optimizing Gaussian Mixture Models}

For GMMs we need to optimize the number of subcomponents each density has.
We should also inspect normalization more and try recalibrating scores as we did for gaussian classifiers with Gaussian Classifiers.

\begin{table}[H] 
    \centering
    \begin{tabular}{||c|c|c|c||}
        \hline
        DCF & Type & \# Components & PCA \\
        \hline
        \hline
        0.346 & Norm. & 3 & No \\ 
        0.348 & Raw & 3 &  10 \\
        0.359 & Raw & 2 &  No \\
        0.360 & Raw & 3 &  No \\
        0.365 & Raw & 2 &  9  \\
        0.375 & Norm. & 3 & 8 \\ 
        0.377 & Norm. & 5 & 9 \\ 
        0.378 & Norm. & 3 & 10 \\ 
        \hline
    \end{tabular}
    \caption{GMM Optimization Results}
    \label{tab:gmmoptimization}
\end{table}

Our two top models are very similar in some ways, like using 3 components, high dimensionality and DCF-wise, but differ for Normalization.

\begin{table}[H] 
    \centering
    \begin{tabular}{||c|c|c|c||}
        \hline
        Type & minDCF & DCF$_{Before}$ & DCF$_{After}$ \\
        \hline
        \hline
        Norm. & 0.316 & 0.346 & \textcolor{green}{\bf 0.324} \\
        Raw & 0.328 & 0.348 & \textcolor{green}{\bf 0.334} \\
        \hline
    \end{tabular}
    \caption{GMM Optimization Results}
    \label{tab:gmmcalibration}
\end{table}

Contrary to what we have seen in table \ref{tab:gmmresults}, here the scores are fairly distant from the minimum possible DCF, and recalibrating the scores did indeed help.

The normalized GMM is the best one, but data does not suggest that in general normalized GMMs are better, so both model will be used later on.

\end{document}